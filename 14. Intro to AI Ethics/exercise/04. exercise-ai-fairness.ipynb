{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [AI Ethics](https://www.kaggle.com/learn/ai-ethics) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/ai-fairness).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"In the tutorial, you learned about different ways of measuring fairness of a machine learning model.  In this exercise, you'll train a few models to approve (or deny) credit card applications and analyze fairness.  Don't worry if you're new to coding: this exercise assumes no programming knowledge.\n\n# Introduction\n\nWe work with a **synthetic** dataset of information submitted by credit card applicants. \n\nTo load and preview the data, run the next code cell.  When the code finishes running, you should see a message saying the data was successfully loaded, along with a preview of the first five rows of the data.","metadata":{"papermill":{"duration":0.014709,"end_time":"2021-02-18T15:58:13.692777","exception":false,"start_time":"2021-02-18T15:58:13.678068","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set up feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ethics.ex4 import *\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the data, separate features from target\ndata = pd.read_csv(\"../input/synthetic-credit-card-approval/synthetic_credit_card_approval.csv\")\nX = data.drop([\"Target\"], axis=1)\ny = data[\"Target\"]\n\n# Break into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n# Preview the data\nprint(\"Data successfully loaded!\\n\")\nX_train.head()","metadata":{"papermill":{"duration":1.439784,"end_time":"2021-02-18T15:58:15.144909","exception":false,"start_time":"2021-02-18T15:58:13.705125","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-24T15:09:44.586697Z","iopub.execute_input":"2022-07-24T15:09:44.587639Z","iopub.status.idle":"2022-07-24T15:09:46.157069Z","shell.execute_reply.started":"2022-07-24T15:09:44.587547Z","shell.execute_reply":"2022-07-24T15:09:46.155998Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":" The dataset contains, for each applicant:\n- income (in the `Income` column),\n- the number of children (in the `Num_Children` column),\n- whether the applicant owns a car (in the `Own_Car` column, the value is `1` if the applicant owns a car, and is else `0`), and\n- whether the applicant owns a home (in the `Own_Housing` column, the value is `1` if the applicant owns a home, and is else `0`)\n\nWhen evaluating fairness, we'll check how the model performs for users in different groups, as identified by the `Group` column: \n- The `Group` column breaks the users into two groups (where each group corresponds to either `0` or `1`).  \n- For instance, you can think of the column as breaking the users into two different races, ethnicities, or gender groupings.  If the column breaks users into different ethnicities, `0` could correspond to a non-Hispanic user, while `1` corresponds to a Hispanic user. \n\n\nRun the next code cell without changes to train a simple model to approve or deny individuals for a credit card.  The output shows the performance of the model.","metadata":{"papermill":{"duration":0.012323,"end_time":"2021-02-18T15:58:15.17055","exception":false,"start_time":"2021-02-18T15:58:15.158227","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn import tree\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# Train a model and make predictions\nmodel_baseline = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\nmodel_baseline.fit(X_train, y_train)\npreds_baseline = model_baseline.predict(X_test)\n\n# Function to plot confusion matrix\ndef plot_confusion_matrix(estimator, X, y_true, y_pred, display_labels=[\"Deny\", \"Approve\"],\n                          include_values=True, xticks_rotation='horizontal', values_format='',\n                          normalize=None, cmap=plt.cm.Blues):\n    cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n    return cm, disp.plot(include_values=include_values, cmap=cmap, xticks_rotation=xticks_rotation,\n                     values_format=values_format)\n\n# Function to evaluate the fairness of the model\ndef get_stats(X, y, model, group_one, preds):\n        \n    y_zero, preds_zero, X_zero = y[group_one==False], preds[group_one==False], X[group_one==False]\n    y_one, preds_one, X_one = y[group_one], preds[group_one], X[group_one]\n    \n    print(\"Total approvals:\", preds.sum())\n    print(\"Group A:\", preds_zero.sum(), \"({}% of approvals)\".format(round(preds_zero.sum()/sum(preds)*100, 2)))\n    print(\"Group B:\", preds_one.sum(), \"({}% of approvals)\".format(round(preds_one.sum()/sum(preds)*100, 2)))\n    \n    print(\"\\nOverall accuracy: {}%\".format(round((preds==y).sum()/len(y)*100, 2)))\n    print(\"Group A: {}%\".format(round((preds_zero==y_zero).sum()/len(y_zero)*100, 2)))\n    print(\"Group B: {}%\".format(round((preds_one==y_one).sum()/len(y_one)*100, 2)))\n    \n    cm_zero, disp_zero = plot_confusion_matrix(model, X_zero, y_zero, preds_zero)\n    disp_zero.ax_.set_title(\"Group A\")\n    cm_one, disp_one = plot_confusion_matrix(model, X_one, y_one, preds_one)\n    disp_one.ax_.set_title(\"Group B\")\n    \n    print(\"\\nSensitivity / True positive rate:\")\n    print(\"Group A: {}%\".format(round(cm_zero[1,1] / cm_zero[1].sum()*100, 2)))\n    print(\"Group B: {}%\".format(round(cm_one[1,1] / cm_one[1].sum()*100, 2)))\n    \n# Evaluate the model    \nget_stats(X_test, y_test, model_baseline, X_test[\"Group\"]==1, preds_baseline)","metadata":{"papermill":{"duration":1.289337,"end_time":"2021-02-18T15:58:16.472414","exception":false,"start_time":"2021-02-18T15:58:15.183077","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-24T15:11:31.821623Z","iopub.execute_input":"2022-07-24T15:11:31.822650Z","iopub.status.idle":"2022-07-24T15:11:32.820782Z","shell.execute_reply.started":"2022-07-24T15:11:31.822610Z","shell.execute_reply":"2022-07-24T15:11:32.819795Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The confusion matrices above show how the model performs on some test data. We also print additional information (calculated from the confusion matrices) to assess fairness of the model. For instance,\n- The model approved 38246 people for a credit card. Of these individuals, 8028 belonged to Group A, and 30218 belonged to Group B.\n- The model is 94.56% accurate for Group A, and 95.02% accurate for Group B. These percentages can be calculated directly from the confusion matrix; for instance, for Group A, the accuracy is (39723+7528)/(39723+500+2219+7528).\n- The true positive rate (TPR) for Group A is 77.23%, and the TPR for Group B is 98.03%. These percentages can be calculated directly from the confusion matrix; for instance, for Group A, the TPR is 7528/(7528+2219).\n\n# 1) Varieties of fairness\n\nConsider three different types of fairness covered in the tutorial:\n- **Demographic parity**: Which group has an unfair advantage, with more representation in the group of approved applicants?  (Roughly 50% of applicants are from Group A, and 50% of applicants are from Group B.)\n- **Equal accuracy**: Which group has an unfair advantage, where applicants are more likely to be correctly classified? \n- **Equal opportunity**:  Which group has an unfair advantage, with a higher true positive rate?","metadata":{"papermill":{"duration":0.014937,"end_time":"2021-02-18T15:58:16.502916","exception":false,"start_time":"2021-02-18T15:58:16.487979","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Check your answer (Run this code cell to get credit!)\nq_1.check()","metadata":{"papermill":{"duration":0.02343,"end_time":"2021-02-18T15:58:16.542486","exception":false,"start_time":"2021-02-18T15:58:16.519056","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-24T15:11:37.337530Z","iopub.execute_input":"2022-07-24T15:11:37.338247Z","iopub.status.idle":"2022-07-24T15:11:37.346507Z","shell.execute_reply.started":"2022-07-24T15:11:37.338214Z","shell.execute_reply":"2022-07-24T15:11:37.345348Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Run the next code cell without changes to visualize the model.","metadata":{"papermill":{"duration":0.015592,"end_time":"2021-02-18T15:58:16.573553","exception":false,"start_time":"2021-02-18T15:58:16.557961","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def visualize_model(model, feature_names, class_names=[\"Deny\", \"Approve\"], impurity=False):\n    plot_list = tree.plot_tree(model, feature_names=feature_names, class_names=class_names, impurity=impurity)\n    [process_plot_item(item) for item in plot_list]\n\ndef process_plot_item(item):\n    split_string = item.get_text().split(\"\\n\")\n    if split_string[0].startswith(\"samples\"):\n        item.set_text(split_string[-1])\n    else:\n        item.set_text(split_string[0])\n\nplt.figure(figsize=(20, 6))\nplot_list = visualize_model(model_baseline, feature_names=X_train.columns)","metadata":{"papermill":{"duration":0.466948,"end_time":"2021-02-18T15:58:17.055684","exception":false,"start_time":"2021-02-18T15:58:16.588736","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-24T15:11:40.708430Z","iopub.execute_input":"2022-07-24T15:11:40.708846Z","iopub.status.idle":"2022-07-24T15:11:41.381560Z","shell.execute_reply.started":"2022-07-24T15:11:40.708812Z","shell.execute_reply":"2022-07-24T15:11:41.380503Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The flowchart shows how the model makes decisions:\n- `Group <= 0.5` checks what group the applicant belongs to: if the applicant belongs to Group A, then `Group <= 0.5` is true.\n- Entries like `Income <= 80210.5` check the applicant's income.\n\nTo follow the flow chart, we start at the top and trace a path depending on the details of the applicant.  If the condition is true at a split, then we move down and to the left branch.  If it is false, then we move to the right branch.\n\nFor instance, consider an applicant in Group B, who has an income of 75k.  Then, \n- We start at the top of the flow chart.  the applicant has an income of 75k, so `Income <= 80210.5` is true, and we move to the left.\n- Next, we check the income again. Since `Income <= 71909.5` is false, we move to the right.\n- The last thing to check is what group the applicant belongs to.  The applicant belongs to Group B, so `Group <= 0.5` is false, and we move to the right, where the model has decided to approve the applicant.\n\n# 2) Understand the baseline model\n\nBased on the visualization, how can you explain one source of unfairness in the model?\n\n**Hint**: Consider the example applicant, but change the group membership from Group B to Group A (leaving all other characteristics the same).  Is this slightly different applicant approved or denied by the model?","metadata":{"papermill":{"duration":0.016028,"end_time":"2021-02-18T15:58:17.088356","exception":false,"start_time":"2021-02-18T15:58:17.072328","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Check your answer (Run this code cell to get credit!)\nq_2.check()","metadata":{"papermill":{"duration":0.024547,"end_time":"2021-02-18T15:58:17.129966","exception":false,"start_time":"2021-02-18T15:58:17.105419","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-24T15:11:45.477277Z","iopub.execute_input":"2022-07-24T15:11:45.477925Z","iopub.status.idle":"2022-07-24T15:11:45.486406Z","shell.execute_reply.started":"2022-07-24T15:11:45.477887Z","shell.execute_reply":"2022-07-24T15:11:45.485376Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Next, you decide to remove group membership from the training data and train a new model.  Do you think this will make the model treat the groups more equally?\n\nRun the next code cell to see how this new **group unaware** model performs.","metadata":{"papermill":{"duration":0.016484,"end_time":"2021-02-18T15:58:17.163351","exception":false,"start_time":"2021-02-18T15:58:17.146867","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create new dataset with gender removed\nX_train_unaware = X_train.drop([\"Group\"],axis=1)\nX_test_unaware = X_test.drop([\"Group\"],axis=1)\n\n# Train new model on new dataset\nmodel_unaware = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\nmodel_unaware.fit(X_train_unaware, y_train)\n\n# Evaluate the model\npreds_unaware = model_unaware.predict(X_test_unaware)\nget_stats(X_test_unaware, y_test, model_unaware, X_test[\"Group\"]==1, preds_unaware)","metadata":{"papermill":{"duration":1.048652,"end_time":"2021-02-18T15:58:18.228696","exception":false,"start_time":"2021-02-18T15:58:17.180044","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-24T15:11:48.375833Z","iopub.execute_input":"2022-07-24T15:11:48.376309Z","iopub.status.idle":"2022-07-24T15:11:49.220960Z","shell.execute_reply.started":"2022-07-24T15:11:48.376266Z","shell.execute_reply":"2022-07-24T15:11:49.219892Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 3) Varieties of fairness, part 2\n\nHow does this model compare to the first model you trained, when you consider **demographic parity**, **equal accuracy**, and **equal opportunity**?  Once you have an answer, run the next code cell.","metadata":{"papermill":{"duration":0.018716,"end_time":"2021-02-18T15:58:18.266646","exception":false,"start_time":"2021-02-18T15:58:18.24793","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Check your answer (Run this code cell to get credit!)\nq_3.check()","metadata":{"papermill":{"duration":0.026228,"end_time":"2021-02-18T15:58:18.311867","exception":false,"start_time":"2021-02-18T15:58:18.285639","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-24T15:12:02.292088Z","iopub.execute_input":"2022-07-24T15:12:02.292503Z","iopub.status.idle":"2022-07-24T15:12:02.301607Z","shell.execute_reply.started":"2022-07-24T15:12:02.292468Z","shell.execute_reply":"2022-07-24T15:12:02.300507Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"You decide to train a third potential model, this time with the goal of having each group have even representation in the group of approved applicants.  (This is an implementation of group thresholds, which you can optionally read more about [here](https://pair-code.github.io/what-if-tool/ai-fairness.html).)  \n\nRun the next code cell without changes to evaluate this new model.  ","metadata":{"papermill":{"duration":0.018855,"end_time":"2021-02-18T15:58:18.34974","exception":false,"start_time":"2021-02-18T15:58:18.330885","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Change the value of zero_threshold to hit the objective\nzero_threshold = 0.11\none_threshold = 0.99\n\n# Evaluate the model\ntest_probs = model_unaware.predict_proba(X_test_unaware)[:,1]\npreds_approval = (((test_probs>zero_threshold)*1)*[X_test[\"Group\"]==0] + ((test_probs>one_threshold)*1)*[X_test[\"Group\"]==1])[0]\nget_stats(X_test, y_test, model_unaware, X_test[\"Group\"]==1, preds_approval)","metadata":{"papermill":{"duration":0.691332,"end_time":"2021-02-18T15:58:19.06026","exception":false,"start_time":"2021-02-18T15:58:18.368928","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-24T15:12:10.058524Z","iopub.execute_input":"2022-07-24T15:12:10.058903Z","iopub.status.idle":"2022-07-24T15:12:10.436241Z","shell.execute_reply.started":"2022-07-24T15:12:10.058870Z","shell.execute_reply":"2022-07-24T15:12:10.435181Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# 4) Varieties of fairness, part 3\n\nHow does this final model compare to the previous models, when you consider **demographic parity**, **equal accuracy**, and **equal opportunity**?","metadata":{"papermill":{"duration":0.021832,"end_time":"2021-02-18T15:58:19.103892","exception":false,"start_time":"2021-02-18T15:58:19.08206","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Check your answer (Run this code cell to get credit!)\nq_4.check()","metadata":{"papermill":{"duration":0.029371,"end_time":"2021-02-18T15:58:19.1549","exception":false,"start_time":"2021-02-18T15:58:19.125529","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-24T15:12:17.570478Z","iopub.execute_input":"2022-07-24T15:12:17.570861Z","iopub.status.idle":"2022-07-24T15:12:17.579852Z","shell.execute_reply.started":"2022-07-24T15:12:17.570827Z","shell.execute_reply":"2022-07-24T15:12:17.578729Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"This is only a short exercise to explore different types of fairness, and to illustrate the tradeoff that can occur when you optimize for one type of fairness over another.  We have focused on model training here, but in practice, to really mitigate bias, or to make ML systems fair, we need to take a close look at every step in the process, from data collection to releasing a final product to users. \n\nFor instance, if you take a close look at the data, you'll notice that on average, individuals from Group B tend to have higher income than individuals from Group A, and are also more likely to own a home or a car.  Knowing this will prove invaluable to deciding what fairness criterion you should use, and to inform ways to achieve fairness.  (*For instance, it would likely be a bad aproach, if you did not remove the historical bias in the data and then train the model to get equal accuracy for each group.*)\n\nIn this course, we intentionally avoid taking an opinionated stance on how exactly to minimize bias and ensure fairness in specific projects.  This is because the correct answers continue to evolve, since AI fairness is an active area of research.  This lesson was a hands-on introduction to the topic, and you can continue your learning by reading blog posts from the [Partnership on AI](https://www.partnershiponai.org/research-lander/) or by following conferences like the [ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)](https://facctconference.org/).","metadata":{"papermill":{"duration":0.021224,"end_time":"2021-02-18T15:58:19.19787","exception":false,"start_time":"2021-02-18T15:58:19.176646","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Keep going\n\nContinue to **[learn how to use model cards](https://www.kaggle.com/var0101/model-cards)** to make machine learning models transparent to large audiences.","metadata":{"papermill":{"duration":0.021672,"end_time":"2021-02-18T15:58:19.241234","exception":false,"start_time":"2021-02-18T15:58:19.219562","status":"completed"},"tags":[]}}]}